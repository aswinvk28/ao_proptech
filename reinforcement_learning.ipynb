{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0378f181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/envs/ao_proptech/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lib.envs.bandit import BanditEnv, ActionSpace\n",
    "from lib.running_variance import RunningVariance\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "import tensorflow_constrained_optimization as tfco\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709e55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"companies_raw_continent.csv\")\n",
    "cols = [\"S. No.\", \"Month\", \"Stage\", \"Continent\", \"HQ\", \"re1\", \"re2\", \"re3\", \"re4\", \"bm1\", \"bm2\", \"bm3\", \"bm4\", \"pv1\", \n",
    "        \"pv2\", \"pv3\", \"pv4\"]\n",
    "df.columns = cols\n",
    "df_outliers = df.where(df['Stage'] == \"TBD\").dropna(axis=0, how='all')\n",
    "df.drop(index=df_outliers.index, inplace=True)\n",
    "y = df.loc[:, [\"Stage\"]]\n",
    "y.dropna(axis=0, inplace=True)\n",
    "X = df.loc[y.index, [\"Month\", \"Continent\", \"HQ\", \"re1\", \"re2\", \"re3\", \"re4\", \"bm1\", \"bm2\", \"bm3\", \"bm4\", \"pv1\", \"pv2\", \"pv3\", \"pv4\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26783d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies = pd.get_dummies(X)\n",
    "y_dummies = pd.get_dummies(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dummies.cumsum(), y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539b2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimizationProblem(tfco.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, labels, predictions, recall_lower_bound):\n",
    "        self._labels = labels\n",
    "        self._predictions = predictions\n",
    "        self._recall_lower_bound = recall_lower_bound\n",
    "        # The number of positively-labeled examples.\n",
    "        self._positive_count = tf.reduce_sum(self._labels)\n",
    "\n",
    "    @property\n",
    "    def num_constraints(self):\n",
    "        return 1\n",
    "    \n",
    "    # we use hinge loss because we need to capture those that are not classified correctly and minimize that loss\n",
    "    def objective(self):\n",
    "        predictions = self._predictions\n",
    "        if callable(predictions):\n",
    "            predictions = predictions()\n",
    "        return tf.compat.v1.losses.hinge_loss(labels=self._labels,\n",
    "          logits=predictions)\n",
    "    \n",
    "    def constraints(self):\n",
    "        # In eager mode, the predictions must be a nullary function returning a\n",
    "        # Tensor. In graph mode, they could be either such a function, or a Tensor\n",
    "        # itself.\n",
    "        predictions = self._predictions\n",
    "        if callable(predictions):\n",
    "            predictions = predictions()\n",
    "        # Recall that the labels are binary (0 or 1).\n",
    "        true_positives = self._labels * tf.cast(predictions > 0, dtype=tf.float32)\n",
    "        accuracy = tf.reduce_sum(self._labels - true_positives)\n",
    "        # The constraint is (recall >= self._recall_lower_bound), which we convert\n",
    "        # to (self._recall_lower_bound - recall <= 0) because\n",
    "        # ConstrainedMinimizationProblems must always provide their constraints in\n",
    "        # the form (tensor <= 0).\n",
    "        #\n",
    "        # The result of this function should be a tensor, with each element being\n",
    "        # a quantity that is constrained to be non-positive. We only have one\n",
    "        # constraint, so we return a one-element tensor.\n",
    "        return accuracy - self._recall_lower_bound\n",
    "    \n",
    "    def proxy_constraints(self):\n",
    "        # In eager mode, the predictions must be a nullary function returning a\n",
    "        # Tensor. In graph mode, they could be either such a function, or a Tensor\n",
    "        # itself.\n",
    "        predictions = self._predictions\n",
    "        if callable(predictions):\n",
    "            predictions = predictions()\n",
    "        # Use 1 - hinge since we're SUBTRACTING recall in the constraint function,\n",
    "        # and we want the proxy constraint function to be convex. Recall that the\n",
    "        # labels are binary (0 or 1).\n",
    "        true_positives = self._labels * tf.minimum(1.0, predictions)\n",
    "        accuracy = tf.reduce_sum(self._labels - true_positives)\n",
    "        # Please see the corresponding comment in the constraints property.\n",
    "        return accuracy - self._recall_lower_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af28c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "def predictions():\n",
    "    return tf.tensordot(yro, mean_r, axes=(1, 0)) - threshold\n",
    "\n",
    "num_examples = 356\n",
    "num_mislabeled_examples = 0\n",
    "dimension = 246\n",
    "class_dimension = 16\n",
    "\n",
    "accuracy_lower_bound = 356.0\n",
    "\n",
    "mean_r = tf.Variable(tf.zeros((dimension, class_dimension)), dtype=tf.float32, name=\"weights\")\n",
    "threshold = tf.Variable(tf.zeros((num_examples, class_dimension)), dtype=tf.float32, name=\"threshold\")\n",
    "\n",
    "input_data = X_dummies.cumsum().values\n",
    "\n",
    "labels = y_dummies.values\n",
    "\n",
    "constant_labels = tf.constant(labels, dtype=tf.float32)\n",
    "constant_input = tf.constant(input_data, dtype=tf.float32)\n",
    "yro = constant_input\n",
    "\n",
    "problem = MinimizationProblem(\n",
    "    labels=constant_labels,\n",
    "    predictions=predictions,\n",
    "    recall_lower_bound=accuracy_lower_bound,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34302c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.executing_eagerly():\n",
    "    # In eager mode, we use a V2 optimizer (a tf.keras.optimizers.Optimizer). A V1\n",
    "    # optimizer, however, would work equally well.\n",
    "    optimizer = tfco.ProxyLagrangianOptimizerV2(\n",
    "      optimizer=tf.keras.optimizers.Adagrad(learning_rate=1.0),\n",
    "      num_constraints=problem.num_constraints)\n",
    "    # In addition to the model parameters (weights and threshold), we also need to\n",
    "    # optimize over any trainable variables associated with the problem (e.g.\n",
    "    # implicit slack variables and weight denominators), and those associated with\n",
    "    # the optimizer (the analogues of the Lagrange multipliers used by the\n",
    "    # proxy-Lagrangian formulation).\n",
    "    var_list = ([mean_r, threshold] + problem.trainable_variables +\n",
    "              optimizer.trainable_variables())\n",
    "\n",
    "    for ii in xrange(1000):\n",
    "        optimizer.minimize(problem, var_list=var_list)\n",
    "\n",
    "    trained_weights = mean_r.numpy()\n",
    "    trained_threshold = threshold.numpy()\n",
    "\n",
    "else:  # We're in graph mode.\n",
    "    # In graph mode, we use a V1 optimizer (a tf.compat.v1.train.Optimizer). A V2\n",
    "    # optimizer, however, would work equally well.\n",
    "    optimizer = tfco.ProxyLagrangianOptimizerV1(\n",
    "      optimizer=tf.compat.v1.train.AdagradOptimizer(learning_rate=1.0))\n",
    "    train_op = optimizer.minimize(problem)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        for ii in xrange(1000):\n",
    "            session.run(train_op)\n",
    "\n",
    "        trained_weights, trained_threshold = session.run((mean_r, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2862711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_hinge_loss(labels, predictions):\n",
    "    # Recall that the labels are binary (0 or 1).\n",
    "    signed_labels = (labels * 2) - 1\n",
    "    return np.mean(np.maximum(0.0, 1 - signed_labels * (predictions > 0)))\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    # Recall that the labels are binary (0 or 1).\n",
    "    true_positives = labels * (predictions > 0)\n",
    "    return accuracy_score(labels, true_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af744a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained average hinge loss = 1.022121\n",
      "Constrained accuracy = 0.873596\n",
      "Error = 0.126404\n"
     ]
    }
   ],
   "source": [
    "trained_predictions = np.matmul(input_data, trained_weights) - trained_threshold\n",
    "print(\"Constrained average hinge loss = %f\" % average_hinge_loss(\n",
    "    labels, trained_predictions))\n",
    "print(\"Constrained accuracy = %f\" % accuracy(labels, trained_predictions))\n",
    "print(\"Error = %f\" % (np.sum(labels - (labels * (trained_predictions > 0))) / len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e2314",
   "metadata": {},
   "source": [
    "### Portion to be Done (Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33096e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 246 # Dimension of state space\n",
    "action_count = 16 # Number of actions\n",
    "update_frequency = 20\n",
    "\n",
    "def discount_rewards(r, factor_model, gamma=0.999):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r,dtype=np.float32)\n",
    "    running_add = 0\n",
    "    f_ = 0\n",
    "    f = np.linalg.norm(factor_model[0],1)\n",
    "    running_add = f\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if (t < (r.size - 1) and r.size >= 2):\n",
    "            f_ = np.linalg.norm(factor_model[t+1],1)\n",
    "            f = np.linalg.norm(factor_model[t],1)\n",
    "            running_add = running_add + gamma * f_ - f\n",
    "        running_add = running_add + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def execute(NOISE_PARAM=4.30, num_epochs=1, max_number_of_episodes=500, reward_sum=0):\n",
    "\n",
    "    class PGCREnv(BanditEnv):\n",
    "        def __init__(self, num_actions = 10, \n",
    "        observation_space = None, distribution = \"factor_model\", evaluation_seed=387):\n",
    "            super(BanditEnv, self).__init__()\n",
    "            \n",
    "            self.action_space = ActionSpace(range(num_actions))\n",
    "            self.distribution = distribution\n",
    "            \n",
    "            self.observation_space = observation_space\n",
    "            \n",
    "            np.random.seed(evaluation_seed)\n",
    "            \n",
    "            self.reward_parameters = None\n",
    "            if distribution == \"bernoulli\":\n",
    "                self.reward_parameters = np.random.rand(num_actions)\n",
    "            elif distribution == \"normal\":\n",
    "                self.reward_parameters = (np.random.randn(num_actions), np.random.rand(num_actions))\n",
    "            elif distribution == \"heavy-tail\":\n",
    "                self.reward_parameters = np.random.rand(num_actions)\n",
    "            elif distribution == \"factor_model\":\n",
    "                self.reward_parameters = (np.array(list(factors.values())).sum(axis=2), \n",
    "                            np.array(list(noises.values())))\n",
    "            else:\n",
    "                print(\"Please use a supported reward distribution\", flush = True)\n",
    "                sys.exit(0)\n",
    "            \n",
    "            if distribution != \"normal\":\n",
    "                self.optimal_arm = np.argmax(self.reward_parameters)\n",
    "            else:\n",
    "                self.optimal_arm = np.argmax(self.reward_parameters[0])\n",
    "        \n",
    "        def reset(self):\n",
    "            self.is_reset = True\n",
    "            action = np.random.randint(0,action_count)\n",
    "            return mel_component_matrix[action], list(factors.keys())[action]\n",
    "        \n",
    "        def compute_gap(self, action):\n",
    "            if self.distribution == \"factor_model\":\n",
    "                gap = np.absolute(self.reward_parameters[0][self.optimal_arm] - self.reward_parameters[0][action])\n",
    "            elif self.distribution != \"normal\":\n",
    "                gap = np.absolute(self.reward_parameters[self.optimal_arm] - self.reward_parameters[action])\n",
    "            else:\n",
    "                gap = np.absolute(self.reward_parameters[0][self.optimal_arm] - self.reward_parameters[0][action])\n",
    "            return gap\n",
    "        \n",
    "        def step(self, action):\n",
    "            self.is_reset = False\n",
    "            \n",
    "            valid_action = True\n",
    "            if (action is None or action < 0 or action >= self.action_space.n):\n",
    "                print(\"Algorithm chose an invalid action; reset reward to -inf\", flush = True)\n",
    "                reward = float(\"-inf\")\n",
    "                gap = float(\"inf\")\n",
    "                valid_action = False\n",
    "            \n",
    "            if self.distribution == \"bernoulli\":\n",
    "                if valid_action:\n",
    "                    reward = np.random.binomial(1, self.reward_parameters[action])\n",
    "                    gap = self.reward_parameters[self.optimal_arm] - self.reward_parameters[action]\n",
    "            elif self.distribution == \"normal\":\n",
    "                if valid_action:\n",
    "                    reward = self.reward_parameters[0][action] + self.reward_parameters[1][action] * np.random.randn()\n",
    "                    gap = self.reward_parameters[0][self.optimal_arm] - self.reward_parameters[0][action]\n",
    "            elif self.distribution == \"heavy-tail\":\n",
    "                if valid_action:\n",
    "                    reward = self.reward_parameters[action] + np.random.standard_cauchy()\n",
    "                    gap = self.reward_parameters[self.optimal_arm] - self.reward_parameters[action]        #HACK to compute expected gap\n",
    "            elif self.distribution == \"factor_model\":\n",
    "                if valid_action:\n",
    "                    reward = np.linalg.norm(\n",
    "                        self.reward_parameters[0][action],1\n",
    "                    ) + \\\n",
    "                    np.linalg.norm(\n",
    "                        self.reward_parameters[1][action],1\n",
    "                    ) * np.random.randn()\n",
    "            else:\n",
    "                print(\"Please use a supported reward distribution\", flush = True)\n",
    "                sys.exit(0)\n",
    "                \n",
    "            observation = marginal_model(action)\n",
    "            \n",
    "            return(observation, list(factors.keys())[action], reward, self.is_reset, '')\n",
    "\n",
    "    running_variance = RunningVariance()\n",
    "\n",
    "    epoch_stats = []\n",
    "    net_actions = []\n",
    "    net_rewards = []\n",
    "    net_scores = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        stats = plotting.EpisodeStats(\n",
    "            episode_lengths=np.zeros(max_number_of_episodes),\n",
    "            episode_rewards=np.zeros(max_number_of_episodes),\n",
    "            episode_running_variance=np.zeros(max_number_of_episodes),\n",
    "            episode_scores=np.zeros(max_number_of_episodes),\n",
    "            losses=np.zeros(max_number_of_episodes))\n",
    "\n",
    "        env = PGCREnv(num_actions = action_count, observation_space = np.zeros((state_dim,1)))\n",
    "        \n",
    "        for episode_number in tqdm(range(max_number_of_episodes)):\n",
    "            states, rewards, labels, scores = [],[],[],[]\n",
    "            done = False\n",
    "\n",
    "            observation, model = env.reset()\n",
    "            factor_sequence = []\n",
    "            t = 1\n",
    "            for state_dim_i in range(state_dim):\n",
    "                done = False\n",
    "                while not done:\n",
    "                    \n",
    "                    state = np.ascontiguousarray(np.reshape(observation[:,state_dim_i], [1,120]).astype(np.float32))\n",
    "                    states.append(state)\n",
    "\n",
    "                    is_reset = False\n",
    "                    score = 0.0\n",
    "                    action, score = least_noise_model(\n",
    "                        model, non_deterministic_hierarchical_clustering(model), \n",
    "                        scaler, MAX_CLUSTERS, NOISE_PARAM, noise_mean, noise_std, \n",
    "                        env=env\n",
    "                    )\n",
    "                    is_reset = env.is_reset\n",
    "                    \n",
    "                    net_actions.append(action)\n",
    "\n",
    "                    z = np.ones((1,state_dim)).astype(np.float32) * 1.25/120\n",
    "                    z[:,state_dim_i] = 0.75\n",
    "                    labels.append(z)\n",
    "                    \n",
    "                    # step the environment and get new measurements\n",
    "                    observation, model, reward, done, _ = env.step(action)\n",
    "                    \n",
    "                    done = is_reset if is_reset is True else False\n",
    "\n",
    "                    observation = np.ascontiguousarray(observation)\n",
    "\n",
    "                    net_rewards.append(reward)\n",
    "                    net_scores.append(score)\n",
    "\n",
    "                    reward_sum += float(reward)\n",
    "\n",
    "                    # Record reward (has to be done after we call step() to get reward for previous action)\n",
    "                    rewards.append(float(reward))\n",
    "\n",
    "                    factor_sequence.append(list(factors.values())[action])\n",
    "\n",
    "                    stats.episode_rewards[episode_number] += reward\n",
    "                    stats.episode_lengths[episode_number] = t\n",
    "                    stats.episode_scores[episode_number] += score\n",
    "\n",
    "                    t += 1\n",
    "\n",
    "            # Stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epr = np.vstack(rewards).astype(np.float32)\n",
    "\n",
    "            # Compute the discounted reward backwards through time.\n",
    "            discounted_epr = discount_rewards(epr, factor_sequence)\n",
    "            \n",
    "            for discounted_reward in discounted_epr:\n",
    "                # Keep a running estimate over the variance of of the discounted rewards\n",
    "                running_variance.add(discounted_reward.sum())\n",
    "\n",
    "            stats.episode_running_variance[episode_number] = running_variance.get_variance()\n",
    "        \n",
    "        plotting.plot_pgresults(stats)\n",
    "        epoch_stats.append(stats)\n",
    "\n",
    "    uniq_actions = np.unique(net_actions)\n",
    "    np.save(dirname + \"/tmp_models/net_actions.npy\", uniq_actions)\n",
    "\n",
    "    print(\"MCTS Coverage: \", len(uniq_actions) / state_dim)\n",
    "\n",
    "    return stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
